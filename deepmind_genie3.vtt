WEBVTT

00:00:00.000 --> 00:00:06.000
By the way, look at this dog. This is amazing. This is insane.

00:00:06.000 --> 00:00:13.000
What was the prompt to create that? Today is a world exclusive of what is, in my opinion, the most mind-blowing

00:00:13.000 --> 00:00:19.000
technology I've ever seen and the most poggers I've ever been. You're not going

00:00:19.000 --> 00:00:25.000
to believe what Google DeepMind showed me in an exclusive demo in London last week. This technology might be the next

00:00:25.000 --> 00:00:30.000
trillion dollar business and might be the killer use case for virtual reality.

00:00:30.000 --> 00:00:37.000
Google DeepMind has been slaying so hard recently that even Gemini Deepth think can't count the number of wins in the

00:00:37.000 --> 00:00:42.000
context window. Let me explain. Today we're going to talk about a new

00:00:42.000 --> 00:02:32.000
class of AI models which are called generative interactive environments. They're not quite like traditional game engines or simulators or even generative video models like VO, but they do have characteristics of all three. They're basically a world model and video generator which is interactive. You can hook up a game controller or any kind of controller for that matter. Deep mind say that a world model is a system that can simulate the dynamics of an environment. The consistency is emergent. There is nothing explicit. the more the model doesn't create any explicit 3D representation. How do you square the circle between like a stochcastic um neural network and yet it has consistency? Right? So I look over here, I look back, I look there again, the thing is back like isn't it a bit weird that a subsey symbolic stochcastic model can give us apparently consistent like solid maps of the world? Do you remember the quake engine in 1996? It required explicit programming of the physics and rules and interactions. But this new generation of AI systems learn real world dynamics directly from video data. You can control an agent in the world in real time. The move towards generative world models was born from the limitations of handcoded simulators. Even their most advanced platform XLAND which was designed for general agent training. It was the frontier for embodied agent training with curriculum learning. But it felt far from the real world. It was almost cartoonike. It could model 25 billion tasks, but it was still handcrafted. It was constrained to the rules of that particular domain. And it was janky. Imagine if you could just generate any interactive world you wanted to train your agents on with a simple prompt. Now, cast your minds back

00:02:32.000 --> 00:04:32.000
to last year when I interviewed Ashley Edwards at ICML. This was the first version of Genie, which was trained on 30,000 hours of 2D platformer game recordings. When we're generating next frames, the objects that are further away are moving more slowly than objects that are closer. And this is a sort of effect that you would often see in games such that you can kind of uh simulate depth. It's something that we also have it, you know, like when we observe things moving, we see things moving slowly when they're further away. So yeah, the model learned that just being able to be that good at understanding the physical world was not something uh we were expecting it to be that good at that quickly. The core innovation of Genie 1 was a spatial temporal video tokenizer that converts raw footage into processable tokens and a latent action model that discovered meaningful controls without label data and an auto reggressive dynamics model which predicted future states. The latent action model, a form of unsupervised action learning, was the core innovation. Genie discovered eight discrete actions which remained consistent across different environments purely by analyzing frametoframe changes in game recordings. This means it knew what jump meant or what move left meant without being explicitly trained on those actions. This was an OMG moment for me. I mean, how was that even possible from training on offline game episodes? Even more surprising was how it seemed to have emergent capabilities like 2.5D parallax. Just 10 months later, Genie2 arrived with 3D capabilities and near realtime performance. The visual fidelity was much higher. Now, it can simulate realistic lighting like the Unreal Engine. you know, things like smoke, fire, water, gravity, pretty much anything you might see in a real game. It even had a reliable memory. You know, you could look away from something and bring it back into view and it would remember the thing. This is Gigachad Jack Parker Holder. He's a research

00:04:32.000 --> 00:06:57.000
scientist at Google Deep Mind in the open-endedness team talking about Genie2 with Demis, no less. This is a photograph taken by someone in our team somewhere in California. And what we then do is ask Genie to convert this into an interactive world. So we prompt the model with this image and Genie converts it into a game-like world that you can then interact in. Every further pixel is generated by a generative AI model. So the AI is making up this scene as it goes along. Exactly. Yes. Someone from our team is actually playing this. They're pressing the W key to move forwards and then from that point onwards, every subsequent frame is generated by the AI. Around the same time last year, uh you'll probably remember this by the way, Deep Mind's Israel team led by Schlomi Frutter showed diffusion models simulating the Doom engine. The system was called game engine. It's almost a meme at this point how Doom runs on, you know, calculators and toasters. But here is a neural network confabulating a Doom game frame by frame in real time. Like look at how it just knows what the health is. You can shoot characters. You can open doors and navigate around maps. You know, occasionally it was slightly glitchy, but this is just unreal. You know, you can just simulate Doom at 25 frames a second on a single TPU. The only limitation, of course, was that it could only do Doom and nothing else. So, last week we waltzed our way into London and Jack and Schlomy gave us a demo of Genie 3. Honestly, I couldn't believe what I was seeing. The resolution is now 720p which is firmly in the good enough territory to suspend disbelief. It's real time. It can simulate real world photorealistic experiences which can continue for several minutes before running out of context. Schlommy had his hands all over VO3 by the way and they seem to have combined elements of the Genie architecture with VO producing something I can only describe as VO on steroids. Unlike Genie 1 and 2, the input is now a text prompt, not an image, which they argued is a good thing from a flexibility perspective, but it does mean that you can no longer take a photo of a real place and generate from there. One of the main features of Genie 3 is that it has a diversity of environments, a long horizon, and promptable world events. Now, on the world events, let's take this ski slope example. We might type in another skier appears wearing a Genie3 t-shirt or a deer runs down the slope and there you

00:06:57.000 --> 00:09:05.000
are. Things just happen in the world. They say that this might be very helpful for modeling things like self-driving cars where you can simulate rare events. But I was left thinking that this is just turtles all the way down. How can we write a process to prompt the potentially infinite number of rare things which could happen in a scene? There was an example they showed of flying around a lake and it was amazing. But I was like thinking where are the birds mate? Like can can you can you type the birds into the prompt? The team believes that we haven't yet had the move 37 moment for embodied agents. You know where an agent discovers a novel real world strategy. They see Genie 3 as the key to enabling that. But the real world constantly surprises us because the real world is creative. Creativity simply means that the tree of things which can happen keeps growing new branches and leaves just keep appearing. Perhaps in the future we might have an outer loop which makes the system more open-ended. But right now in my opinion Genie 3, like all AI gives you exactly what you ask for in the prompts and isn't creative on its own. Currently, the system only supports a single agent experience, but imagine how cool it would be if you could extend that to a multi- aent system. Apparently, they are working on that. I mean, personally, I'm most excited about a new modality of interactive entertainment. You know, just imagine YouTube version two. Deep Mind sees the main use case of being able to train robotic simulations as being the real gamecher. This seems plausible to I mean like the miracle of human cognition or in brains is that we have evolved to simulate the world without direct physical experience which is expensive. This is basically the same idea right why train in the real world if we can just simulate any possible scenario in a computer just like that black mirror episode. Here's a couple of examples they gave of using simulated environments to train an agent to do some specific language tasks. Now, with Genie2, they said they were happy if it was consistent even for 20 seconds. But now, when you notice something inaccurate, it's very surprising. The key thing is that it now extends beyond the prediction horizon of the average human, and the glitches are getting harder and harder to spot. They said that Genie 2 wasn't actually real time. You had to wait a few seconds between taking different actions. you know, it was low resolution, had limited memory, you know, I mean, it was superficially really good, but it didn't look particularly photo realistic. Genie 3 changes all of that. So, Genie supported around 10 seconds of generation. Genie 2 around 20 seconds. Genie 3 is able to simulate interactive environments for multiple minutes. This time around, they were a little bit more tight- lipped around the architecture. They wanted to focus on capabilities in the interview, and that's fair enough. I mean, it's understandable given that this is potentially a trillion dollar business and Zuck will be sniffing around like a truffle hound. My my biggest concern with this is that as soon as Zuck gets wind of this, he is going to be getting out his checkbook. He's going to go straight to Jack and Schlomi and he's going to be like, "Come on, boys. $100 million. Come to work for me." Um, Zuck, mate. Seriously, no. Don't do it. These guys, they they're doing God's work over here. You need to just let them let them do what they're doing. You can make it yourself if you want, Zach. Leave them alone. I should say I did joke at the end of the interview that if you are learning Unreal Engine right now, you might want to pivot to a different career. But the Google guys were quite grounded. They argued that this is a different type of technology. There are pros and cons, you know, which is fair. I should stress that as amazing as this technology is, it's still a neural network and it still has many important limitations. Certainly though, just imagine how easily you could generate an interactive motion graphics with this technology. You know, that's something that Unreal Engine has been leaning hard towards in version 5.6. So, do I need to fire my motion graphics designers? Victoria, will users be able to use this? Not anytime soon. This is still a research prototype and given the obvious safety concerns, they're going to open this up progressively through their testing program. One question did come up in the press conference yesterday though like could it generate an ancient battle and Schlomi said that it's not trained on that kind of data wouldn't be able to do that yet. So I mean certainly not a specific historical battle anyway. So it does sound like there are still some limitations. How can a system like this ever be fully reliable? Well they did say that with better models the trend is that they get more and more accurate. The glitches become fewer and they expect to see further improvements. You know, there's this annoying phrase like this is the worst the model will ever be, but even as I said, they can generate some edge cases using a whole bunch of prompt augmentations, but it might just be turtles all the way down. You know, how do you come up with all of the rare black swan events that might happen? So, what data was it trained on? They were quite cy about this as well. It's probably safe to assume that it's been trained on all of YouTube and lots more besides that.

00:09:05.000 --> 00:15:11.000
How much compute does this thing need? Well, I asked them that and they were a little bit vague about it. Um, they said that it ran on their TPU network. So, I'm inferring from that it needs a crap ton of compute. However, I can say that it was demoed in front of me. It was very responsive. You put a prompt in, it thinks for about 3 seconds, and then you're just in and it just works. They also mentioned some cool stuff about how, you know, like Genie can be used to train agents as we said, but the agents themselves could be used to better train Genie 3, creating this virtuous cycle of iterative improvement. If you're in a world walking around and say you go to cross the street, you sort of check the cues of the of the drivers, for example, maybe there's not a crosswalk um and you need to know when to stop. You can see that they're slowing down, so that's when you would go and the other agents should be simulated in that fashion. Gen3 and other similar models would be impossible without at least some human feedback in the training loop or the data curation or the evaluation. Prolific is a human data platform and they are sponsoring this video today. My name is Enzo. Um I uh I work at Prolific. I'm the VP of data and AI. I support everything from AI data research uh and the likes. Um, for those unfamiliar, Prolific is a human data platform for working with everything from academic researchers, but also small and large players in the AI industry. Visit prolific.com. Yes, this is the demo where they've got G3 memory test on a blackboard. you see there's like an apple and a cup and then you kind of like go out you you look out the um the window you see there's like a few cars and the purpose of this test is to kind of say they've got such a long context window you know similar concept to a large language model that it still remembers all of the things that it generated you know even if it's minutes ago we've got the the blackboard over here we look up and there it is it remembered it gen3 memory test I've also noticed that this model is is even better than V3 at things like text. It's because you you would think that they would they would dumb down the model to make it interactive and to make it this sophisticated. But even as a video generator model, it seems almost better than V3 for doing a whole bunch of stuff. All right. So, I'm Shomi Fer. I'm a research director at Google DeepMind. I'm the Veo Collid. Um and um I'm basically working at Google for about 11 years. um recently on diffusion models in the various modalities, image uh video and we'll tell you more about what we're working on right now. Hey, I'm Jack Parker Holder. I'm a research scientist at Google Deep Mind in the Open Endlessness team. Um originally working on open-ended learning and open-endedness and more recently working on world models. We are here at Google Deep Mind in London and you guys have just demoed to me something which I think I'm more impressed with this I think than anything I've seen probably ever before. I think it's a paradigm changing moment. Shomi, can can you can you tell us a little bit about this new version of Genie?

00:15:11.000 --> 00:20:00.000
Sure. So, Genie uh is our most capable world model. And by a word model, what we mean is basically a model that is able to um predict how an environment would evolve and also how different actions of an agent would affect this environment. So with um Genie3 we are able to basically push the capabilities of a world model um to a new frontier um that means high resolution much longer horizon and better consistency and all that real time in real time basically allowing wherever if whether it's an agent or a person that interacts with the system to walk around it navigate it affect it while the generation happens in real time. Genie 3 is just ridiculous, right? It's just on a completely different level. But maybe we should just contextualize that around Genie2. So what was Genie 2? Yeah, it's a great question. Um, so Genie2 was sort of the culmination of two years of research in what was quite a new area which is foundation world models we called it at the time. So essentially in the past world models had modeled a single environment. So the canonical world models paper in 2018 from David Haren Huber modeled the car racing environment uh which is a major environment and it could just model that one environment. It could predict the next states given any actions in that one world. We've seen with the dreamer series um also from from Google mind dagar uh with Atari games and other kind of environments as well but no one had ever done something that could create new worlds. So with Genie 1, the real novelty there was that we had a model that for the first time could be prompted to create completely new worlds that didn't previously exist. But that being said, they were fairly rudimentary. Like they were um low resolution. You could only play with it for a couple of seconds. Um so agents couldn't really learn long horizon behaviors that wanted to and the diversity was still fairly constrained and it required some form of image prompting. With Genie2, we really pushed that um um to the next level, right? So we trained it on a much light larger distribution of 3D environments. We moved to 360p from I think 90p before. So it was it was more like close to what we see now, but it was still sort of scratching the surface because we didn't really know that this approach could scale the way we've seen other methods have. So we wanted to really like test this from a research standpoint. Um but then I think for this year we wanted to really take that to the to the next level. Uh and that's what we think we've done. Yes. And it's now 720p. It's interactive. So Genie 2 wasn't wasn't interactive. It was it wasn't fast enough. And you know, Steve Jobs said there's something magic about the touchcreen, right? There's something magic about it. And of course, the magic happens when it's interactive. And some of the demos you showed me were just just insane.

00:20:00.000 --> 00:25:00.000
Right. So um photo realistic I mean it's kind of like a fusion of VO I suppose that you can now understand the real world and you can build essentially a foundation model for the real world which is interactive that's mind-blowing and and just tell me about some of the examples you showed yeah so I think what you said about veil or more generally about video models is right like we there is a way we can think about them as somewhat a world model but it's not really it doesn't allow us to actually um navigate or interact with it um completely interactively. And I think that's that's one of the limitations of of video models that with Genie Free we're trying to address. Um and basically in the examples that you've seen um we're able to because Genifreeze generates the experience and what the what we see frame by frame. Um, it lets the user or the agent that is using it basically control where it wants to go in every like in a very low latency that allows um uh basically exploring the environment and and creating new trajectories that are not predefined like video models. Um so in the examples that you've seen for example um you can see the character or the agent in this video moving around maybe going back to the place there already been in uh before and everything remains consistent and I think that's a very remarkable property or capability of the model the ability to preserve and the consistency of the environment along very long trajectories. Yes. and and even Genie2 had some kind of object permanence and consistency, but nowhere near as much as we have now, but we'll come back to that in a second. We can't say too much about the architecture for Genie 3. But in in Genie2, there was a um an ST transformer, so a special temporal transformer, which was conceptually quite similar to like a VIT, and there was um a latent action model, which means even from like, you know, non-interactive data, you could infer some low cardality action space, and then those went into a dynamics model. I I think we can what we can say about the architecture that you know might be interesting is that definitely because of the interactive nature of the problem or the setup then the model is not regressive. So what it means that it means that um the model generates frame by frame and has to refer back to everything that happened before right. So if for example uh we're walking around some auditorium or what some some other environment um basically if we rever if we revisit an a place that we already been to uh the model has to look back and and understand that this information has to be consistent with what's happening uh um in in the next frame. Um so I think the interesting the interesting point here is that everything here like the consistency is emergent. There is nothing explicit. the more the model doesn't create any explicit 3D representation and I un unlike you know methods other methods like nerves and goian splatting um so I think that's that emergence kind like capabilities very interesting and surprising for us yes yes and and even G2 had emerging capabilities like parallax and you know it could model certain forms of lighting and so on but this just blows my mind you're you're involved in that Doom simulation last

00:25:00.000 --> 00:30:00.000
Yeah. And even that just blows my mind. Right. So, we all played Doom in 1993. It was one of John Carmarmac's finest. And now you're saying that I mean certainly the work that that you folks did last year. You've got a neural network model which is subsemb. So there's no explicit model of the world. You don't know where the doors are. You don't know where the lakes are, where the maps are, and so on. You just kind of take a, you know, a sample, a traversal through this space, and and it just produces the game in pixel space. I mean that's yeah you know this is really you know I've been playing games obviously including you know Doom and others and um I also worked some on on game engine development at some point uh very early in my teens and I I think what what I really like about this project is that we now are now able to run models that actually generate consistent 3D environments as in you know game engine and the Doom simulation and they run on GPUs or TPUs uh while in the past we were running you know this like uh game engines on the same hardware. So I think there is it's really something very interesting and it kind of closed this circle for me. Um and in particular in the case of game engine we try to push um on the real time interactive kind like aspect. So uh we basically said okay would uh a diffusion model be able to simulate a game environment end to end with nothing explicit no code nothing except for actually generating the pixels and getting the inputs from the user and you know we weren't sure if it's going to work so I think there is like with this kind of research we try and it doesn't work and then it all of a sudden something happens and it we we we see that it does work and that's a very rewarding moment and I think in this case um once people saw Oh, I think this was really even the the the reception of that was a bit surprising because there is something about the real-time interactive um capability that really sparks the imagination of oh I can actually walk into this environment maybe generated environment and actually experience it right so I think this was a moment um that um later when I think about it like we were kind of like um excited about the the real-time nature of of the simulation and We really wanted to bring it to higher quality, more general purpose uh simulations. So Jack, I mean one of the um the million-dollar questions is, you know, like even with a language model, it's it's stochcastically sampled, you know, with this temperature parameter. Same thing here. I mean, with with Genie2, the dynamics model is using this masked git and it was it was run iteratively. And how do you square the circle between like a stochastic um neural network and yet it has consistency? Right? So I look over here, I look back, I look there again, the thing is back. Like isn't it a bit weird that a sub symbolic stochastic model can give us apparently consistent like solid maps of the world?

00:30:00.000 --> 00:35:00.000
That's a really good question. Um I think probably similar to language models that there are some fundamental things about the world that you want to remain consistent. So with a language model um I think even though as you said they can be like sarcastic models if there is things that stated as facts in their in their context they will still probably recall them correctly right whereas new things are where they maybe have more degrees of freedom to to change things like that. So I'd imagine in a world like a genie generated world if you were to move around then maybe new things would be have some degree of of um stocasticity to them right but then once they've been seen once then they should be consistent from that point forward because the model knows when to use this stocasticity um and this is kind of an emergent property from um the scale that we train at.

00:35:00.000 --> 00:40:00.000
Yes. And we we'll save the emergent discussion. I was I was just telling the guys about my conversation with David Krakow the other day but maybe we won't go there. Um but um the other really interesting thing is so you know you said David Har you know 2018 with Schmid Huba the world models thing and um uh show me in the presentation you defined a world model as essentially being able to simulate the dynamics of something right if if a world model simulates the dynamics of of a system how could you for example how could you measure that so I think it's very hard to exactly measure the quality of world models in general and I I think when it comes especially to visual um to visual generation if it's image models and and you know generative models in in general very difficult to to measure their quality because it's somewhat of fit is very subjective right so I think for LLMs actually we're in a better place because we can measure their performance first of course there is perplexity just a next token prediction problem but later on we actually care about how they operate for for the task that we care about right so we measure for example downstream like performance on various tasks. But for when it comes to world models and today we focus mostly on the visual aspect, right? So it's it's important to highlight that the world is more than just visuals, right? Um but but again uh for Genie we're focusing more on that um because a lot is captured in the visual interaction um of the world. Um so measuring how well a model is is doing really depends on the context and and also on how we want to use it later. I think that's something we have to keep in mind when we evaluate things um in models. So we have in mind one particular application that we think is really key and that's to be able to actually uh train and and let a AI agents interact with simulation environments. And I think that's something that um you know I'm coming more from this kind like a kind like maybe simulation background but not so much from the um maybe training agents in simulation environments that's not so much was wasn't my my original background but you know I think through the interaction with you know other people in deep mind that are kind like exploring that for a long time I was really um you know over the last few years I kind of came more and more to realize how much potential there is in in that that Because if we really think about it, AI would be limited by the ability to perform physical experience experiments, right? Because imagine that you want to develop a new drug or a new um collect treatment. Um you cannot really do it in uh in the real world if it takes months for every step in the way right and the same we can think about you know if we want to learn how to assemble something then again if I have to train the robot in um in the real world it might take very long. So that's why the simulation of the real world is really key and that's what we hope we kind like push a bit farther with G3.

00:40:00.000 --> 00:45:00.000
Yes. Very exciting. I spoke to a startup recently and they they sketched out this future where we'll have um essentially um a model platform where people doing robotics can download the policies you know so I'm I'm in a factory and I need a policy for doing this particular thing. But of course, you know, they imagined that it's so scarce, it's so difficult to get real world data that, you know, there would be a marketplace and everyone would train their own policies and they would like sell it to other people on the market. This is a slightly different vision. You're saying that now we have a a world foundation model and essentially I could say, well, in this situation, I need to have a robot policy for doing this particular thing. So, I can just spin off a job. I can create the policy and and away we go. So, is is that roughly correct? I think that is kind of the vision that we have. So I think in robotics in particular there's a lot of focus on deploying robots in somewhat constrained settings right so it might be for example in someone's apartment that's very staged right um almost as stage as a podcast recording you know got like all these support staff watching around this robot achie a achieve one goal right and from a control um perspective it might be very impressive but in terms of the stoasticity of the world that it's in it's very limited right um and if we look at simulated simulation environments they might accurately model physics but they definitely don't model things like weather or other agents or animals or these kinds of things right um whereas a model like Genie 3 because it's it has world knowledge that world knowledge extends beyond physics actually to also the behavior of other agents and as we showed you in that example at the beginning with the the world events that we can also inject right you can actually prompt to have you know another agent crosses in front of you or like you know we had a herd of deer run down the ski slope or something like that and I think these are the kind of things that for robots to be deployed at large scale in the real world. The real world is fundamentally populated by people uh and other agents and this is something that we can gain from training on this general purpose world model uh that we just have no other approach I think to scalably get this data in a safe way as well right because the safety is a critical element of this that we can simulate things uh in a realistic way without having to actually um deploy agents in the real world.

00:45:00.000 --> 00:50:00.000
Yes. And that was a very important detail. So you can you can put a prompt event in and you gave me an example of there there's a skier going down the slope and then here's a guy with a Gemini t-shirt. And I guess what I'm thinking about here is if we did train these robot policies, we would need to do probably some kind of curriculum learning and some kind of diversity. So, you know, we would start off with a simple environment and then we'd add the guy with the Gemini t-shirt and then there'd be a car coming along and maybe in reality there would be some kind of meta process, you know, creating some gradient of complexity and, you know, diversifying environments. I love that Ken Stanley paper, you know, the poet paper doing something like that. But is is is that like a fairly reasonable intuition? So I think it's still early to say exactly how u word models um like genifree will be actually used for AI research. I think we can only kind of directionally say um we I in general I think we we still um we see it in also in other generative models that there are some capabilities that we actually discover right and we don't necessarily know that they're there and then through the interaction development we're actually seeing them emerge for example you know we we just recent like a few days ago we then we we kind like shared that um you can write like some text on a on a photo and provide it to val and it just like it reads the the the text and it follows also the spatial instructions right and I think that's that's for example something that we didn't necessarily explicitly train them all to do but it it's capable of doing and I think here as well the capabilities of Genie free uh that we're exploring are still uh we still discover new things and I think that's something that we hope um that by um first by having more like you know testers and external testers that we already shared some some uh uh like we basically previewed the model to and give us feedback. So we hope that through this kind like engagement with the community um and we can better see how those models will be useful um and that's something that I expect to take some time um as we we basically try and understand the best application.

00:50:00.000 --> 00:55:00.000
You know, I'm a huge fan of open-endedness for for example, and um certainly at the moment when we prompt models, if we're quite generic, you know, in in in what we put in the prompt, then we tend to get quite simplistic answers. So, you know, a lot of um people doing computer graphics when they prompt image models, they they have so much specificity and they deliberately take it, you know, on onto the tail of the distribution so they get something that's novel and interesting and so on. and and the real world just always produces a sequence of artifacts which are novel and interesting, you know, like you get random NPCs walk onto the onto the screen and cars go in and so on. And is my intuition correct that that at the moment um as good as it is with with um with Genie 3, you you tend to get quite a specific scene and you don't have like random kind of planes flying over and sort of, you know, just random things happening. Yeah. So that's a really a good intuition, right? So it definitely is the case that um the model is very it's very aligned with the text prompt that that it's given. So therefore there is a lot of emphasis placed on the quality of the text prompt to describe the scene. Um but I I actually wouldn't see that as an limitation. I would see as a strength, right? So firstly it means that there's actually a lot of human scale still involved to create really cool worlds, right? And you see some of the examples we showed you. We have some very talented people that can do amazing things with these models, right? and and there is actually a lot of value add there to do that right so it actually uh is a tool that can really amplify already creative humans um in new ways and I'm definitely not the best at doing this right and I can tell you that it does it is really impressive when someone is able to do that but on the on the flip side from the the the agent perspective as well right so when we're talking about designing environments for agents um and and you referenced poet which was um for me like poet and dwell models were the two papers that I just thought were eventually on a collision course right in That's basically why I started my research career. Um, and I think poet was fundamentally limited because of the the environment in coding being an eight dimensional vector. Um, but also the fact that there was no really any notion of interestingness as well, right? And in your recent interview with with Jeff, right, he's obviously talked about how this problem is largely now solved with foundation models, right? So these foundation models can not only define what's interesting based on standing on the shoulders of of human knowledge, right? But they can also steer the generation of worlds in things like Omni Epic um um to do this. And that's in that case it's done through code. But here we have text as a substrate as well. So in theory this these kind of open-ended algorithms that use language could actually be quite strong places to have these kind of like notions of interestingness and agents steer tasks through that space as well.

00:55:00.000 --> 00:58:12.000
Yeah, I think I think this is the fundamental thing because certainly with um creative models at the moment. Um like weirdly counterintuitively you need more skill to make it do something interesting than you did before. Like the average creative process now for someone designing a thumbnail on YouTube is they they mix together you know they might use the contact model, they might use an upscaler, they might then like you know use another image generator model. you get this huge kind of compositional tree of operations that happen and it's very very highly skilled because a lot of the kind of um structure for constraining the generation of these models still comes from our own abstract understanding of the world and this is kind of what Kenneth Stanley was saying in a sense he was saying that we have this understanding of the world which is constrained by things like symmetry and you know various different rules and we then kind of we we hint to the model we we constrain the model in the prompt using those things. Would the models ever be able to do that without the humans needing to prompt them? So I I think what's interesting is that eventually what we find like what humans find interesting and worth um you know maybe watching or investigating or researching it's it's eventually being defined by people and and I think in the case for example if it's video uh if it's it's video generation for example then we see that people go and and find ways that maybe we weren't like they use the tool that we put in front of them to generate new things. So, for example, we have people try to cut like we have the ASMR videos of people cutting, you know, fruits made of glass, right? Which it's not something you can do in the real world. And the novelty comes from from the prompt basically. Um, I think that's what you're alluding to. Um, so I think in in the case of we're still in a similar place I would say when we think about the world models because you have to provide the description of the world that you want to to maybe walk in into an experience but some elements are not like would kind like emerge from and we will be inferred from the prompt that you provide. Right? So you can maybe write a very short prompt but still the world will have much more richness. So I think there's a question of where does this rich richness coming from and I think where it's different maybe levels of of this the the ability of models um to to bring this rich richness into your experience but I think it's kind of like over time we see that it becomes more and more higher and higher and little uh information provided by users can actually generate very rich videos or experiences. So I would say it's not like a it's a bit of a evolving answer. I would say like over time I expect that more inputs to the model or you can think about it like the the the person is providing a seed and from that seed we can maybe generate more like more elaborate descriptions and finally an experience. So I don't think of it as like a one-step um process but more of like a series of of um creative steps or each one of them can be can happen by can be done by a person or by an AI model and together they generate maybe something new. Yeah. And and that's what we're seeing play out on Twitter that you know because the creative process is like you know generate discriminate generate discriminate and we mimemetically share all of the prompts that work and that's why we've just created this beautiful fogyny of creative artifacts that are exploring the you know the the space of of these models which is beautiful and I'm thinking about the future. I mean I know you probably can't speculate about this but this could be the next YouTube it could be a new form of virtual reality. You know, in philosophy, there's this thing called the experience machine where you you plug yourself into this better than life matrix simulation, and no one wants to leave the experience machine because it's better than real life. But we could co-create something like that, right? We could we could have it could be on a on a phone or a virtual headset, and we could create these worlds and portals between the worlds and it would just be a neverending simulation. Yeah. So, it's a that's a great question. So I mean going back a few steps I think another really inspiring sort of thought experiment in this space before the generative models really became capable was something like pickreeder right and so in that case it was a very simple idea right it was just you know evolving some some im images basically um and some quite surprisingly creative things emerged from that experiment that I don't think um many people would have expected right so you had these beautiful um beautiful images basically emerging uh to use the word emerging again emerging emerging from just evolving um evolving um user preferences over time, right? And we definitely see modern analogies of this like you described with you know social media platforms sharing prompts and people generating ideas and then it emerges in different ways or goes in different ways like um like the VO ones with people generating standup for example and then suddenly there's tons of exciting content in that space and I think that it's definitely fair to say that what we've done with Genie 3 is create another form another platform or type of model where this kind of creativity could happen and it could also lead to some unexpected exciting things. Um, but I don't think we can speculate too much at this point exactly what those will be. Um, other than say that it it should be interesting and humans will likely do cool things with it. Yes, I I was discussing with Kenneth the other day whether because he's a big fan of um, you know, neuro evolution and I think he's leaning towards the the evol you know like creating an algorithm that represents evolution in of itself as being the way to, you know, explore interesting fogynies. And for me, pigreeder was like a kind of um supervised human imitation learning. So it was almost like a reflection of the constraints and the cognition that we have. And I lean externalist a little bit. So I I think that a lot of se semantics is about this embodied physical interaction with the world and that you know just via osmosis perhaps gets represented in our brains. But do do you have a position on that? you know, do do you think that just just pure neural networks simulating the world could could understand the world in the same way? So maybe first to like about kind like the immersion or the the maybe potentially using you know this kind of kind of models for actually you know being immersed in it like I think this is we're still very far like I think it's really I said before that I think the visual aspects are pretty much primary right we're generating pixels and you know with free we also added audio but our embodied existence is so much more than that and I think sometimes we you know it's it's getting lost Right? Like because eventually we like as as people we feel a lot we walk around. We have other senses. We we have this sense of like where I am right now and and and I think that and and of course the physical interaction which is also applicable to robots, right? So there's still a large gap between where we are right now and where and building you know real full simulation of the world that can actually provide all of the information to an embodied agent. So I think there is definitely a gap there. Um that is interesting but it does show that we're still very far you know in those in that regard. Um and but I think as as Jack said basically building those kind of like experiences we do we do see people uh try to com to build experiences together and kind like explore worlds together and I think that's a very interesting uh direction for us. Yes. Yes. Yes. Um yeah so many things to talk about there. I mean I suppose one one important step is this multi- aent simulation thing right so quite a few people have spoken about this certainly David Krakow he said that a lot of um you know emergent intelligence is about coarse graining when you have these systems that can you know through a variety of tricks accumulate information over time. So um you know eventually like we developed a nervous system and culture and language and that allowed us to commun you know to accumulate information sort of like you know um transgressing the the the hardware the DNA um evolution speed. So it's evolution at light speed and Max Bennett spoke about that in his um brief history of intelligence how you know like a lot of the evolution of the brain in culture was about the propagation of information without needing to have direct physical experience. So we can implicitly share simulations with each other. So um you know when we start to build these multi-agent simulations do you think that similar things might emerge where you know um almost irrespective of the life span of an individual agent that the system could accumulate information and develop forms of agency and dynamics that like simple systems couldn't. Uh that's a really good question. And so I think the way I would see it from the standpoint of Genie 3 where it is right now is that it's sort of um it's a multi-agent world, but that's only controllable in a in a in a single agent setting, right? So a lot of the multi- aentness about the world is sort of baked into the the simulation around you. Um they're almost like additional characters in the world rather than being like controllable agents. You can control them if you wanted to with the world events, right? So you could actually um control what the other agents are doing but otherwise it's always kind of implicit in the weights. Um and what you see is that there is some sort of like natural behavior of them. So if you walk through a c crowd people will move out the way for instance. Um if you're if you if you create a driving world then when you drive around the other cars move in a sensible fashion. Um, and go to go back to your actual question, I think you're saying almost the system can almost like bootstrap from itself and learn um to to learn to learn um sort of across the different agents in the system. I think the way I would see it right now is more that the the model's sort of knowledge of of human behaviors can distill into the the egocentric agent. Uh and that's actually something quite powerful that we haven't really got with any other simulation tool, right? Because if the other agents are moving around sort of in a way that we do, um then I think it might even be a way of of our embodied agents learning things like theory of mind because they know for instance that if you're in a if you're in a if you're in a world walking around and say you go to cross a street, you sort of check the cues of the of the drivers for example, maybe there's not a crosswalk um and you need to know when to stop. You can see that they're slowing down. So that's when you would go and the other agents should be simulated in that fashion. So actually you can learn these kind of cues that you can't really learn any other way other than being deployed in the real world and that obviously has safety um safety risks and probably wouldn't be an advisable thing to do with an agent that's learning from its own experience. Um so what we think with this kind of model is that agents can really learn these sort of social cues things like theory of mind how to operate within humanike other agents but it's not the case that the model itself is then learning back from the agent that's collecting experience. That might be a future step, but not something we've really considered in this work yet.

Amazing guys. This has been an absolute honor. Thank you so much for coming on. And for folks at home, if you're developing on Unreal Engine, might be time to, you know, Yeah. Anyway, cheers.